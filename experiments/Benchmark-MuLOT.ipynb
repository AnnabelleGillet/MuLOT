{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:22:53.097481Z",
     "start_time": "2021-02-20T18:22:47.526Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.1`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:22:54.035197Z",
     "start_time": "2021-02-20T18:22:48.112Z"
    }
   },
   "outputs": [],
   "source": [
    "val currentDirectory = new java.io.File(\".\").getCanonicalPath\n",
    "val path = java.nio.file.FileSystems.getDefault().getPath(s\"$currentDirectory/lib/sparktensordecomposition_2.12-0.1.jar\")\n",
    "val x = ammonite.ops.Path(path)\n",
    "interp.load.cp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:22:54.660926Z",
     "start_time": "2021-02-20T18:22:48.731Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.mllib.linalg.distributed.{ExtendedBlockMatrix, IndexedRowMatrix}\n",
    "import org.apache.spark.mllib.linalg.distributed.ExtendedBlockMatrix._\n",
    "import tensordecomposition._\n",
    "import tensordecomposition.CPALS._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:22:57.206982Z",
     "start_time": "2021-02-20T18:22:49.406Z"
    }
   },
   "outputs": [],
   "source": [
    "implicit val spark = {\n",
    "    val MAX_MEMORY = \"126g\"\n",
    "    SparkSession.builder()\n",
    "        .config(\"spark.executor.memory\", MAX_MEMORY)\n",
    "        .config(\"spark.driver.memory\", MAX_MEMORY)\n",
    "        .appName(\"BenchmarkSparkCPALS\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "}\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sparkContext.setCheckpointDir(\"Checkpoint\")\n",
    "import spark.implicits._\n",
    "\n",
    "spark.sparkContext.getConf.getAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:22:57.602893Z",
     "start_time": "2021-02-20T18:22:54.434Z"
    }
   },
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "\n",
    "val tensorsFiles = new File(\"sample_tensors\").listFiles\n",
    "    .map(_.getName)\n",
    "    .filter(f => f.startsWith(\"tensor\") && !f.contains(\"clusters\")).toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:23:05.609021Z",
     "start_time": "2021-02-20T18:22:56.038Z"
    }
   },
   "outputs": [],
   "source": [
    "val tensors = (for (tensorFile <- tensorsFiles) yield {\n",
    "    val name = tensorFile.replace(\".csv\", \"\").replace(\"tensor_\", \"\").split(\"_\")\n",
    "    val nbDimensions = name(0).toInt\n",
    "    val size = name(1).toLong\n",
    "    (tensorFile -> Tensor.fromIndexedDataFrame(\n",
    "        spark.read.option(\"header\",true).csv(s\"sample_tensors/$tensorFile\").dropDuplicates(for (i <- 0 until nbDimensions) yield s\"d$i\"), \n",
    "        (for (i <- 0 until nbDimensions) yield size)toList))\n",
    "}).toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:23:05.945386Z",
     "start_time": "2021-02-20T18:23:02.268Z"
    }
   },
   "outputs": [],
   "source": [
    "import scala.collection.mutable.{Map => MMap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T21:05:49.040615Z",
     "start_time": "2021-02-20T19:44:41.816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var timeCPALSCoordinateMatrix = MMap[Int, MMap[Int, MMap[Double, Int]]]()\n",
    "\n",
    "for (dimension <- 3 to 3; \n",
    "     size <- List(100, 1000, 10000, 100000);\n",
    "     sparsity <- List(1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10).reverse) {\n",
    "    val fileName = s\"tensor_${dimension}_${size}_${sparsity}.csv\"\n",
    "    if (tensors.contains(fileName)) {\n",
    "        println(fileName)\n",
    "        val nbIterations = 5\n",
    "        var endTime = 0\n",
    "        for (j <- 0 until nbIterations) {\n",
    "            spark.catalog.clearCache()\n",
    "            val _t = tensors.get(fileName).get\n",
    "            val tensor = new Tensor(_t.data.cache(),\n",
    "                _t.order,\n",
    "                _t.dimensionsSize,\n",
    "                _t.dimensionsName,\n",
    "                _t.dimensionsIndex)\n",
    "            \n",
    "            tensor.data.count()\n",
    "            val startTime = System.currentTimeMillis()\n",
    "\n",
    "            tensor.runCPALS(3, 5, 0.9999999999999, true)\n",
    "\n",
    "            endTime += (System.currentTimeMillis() - startTime).toInt\n",
    "\n",
    "            println(\"Execution time: \" + (endTime / 1000) + \"s\")\n",
    "        }\n",
    "        val finalTime = (endTime / nbIterations).toInt\n",
    "        var dimMap = timeCPALSCoordinateMatrix.getOrElse(dimension, MMap[Int, MMap[Double, Int]]())\n",
    "        var sizeMap = dimMap.getOrElse(size, MMap[Double, Int]())\n",
    "        sizeMap = sizeMap + (sparsity -> finalTime)\n",
    "        dimMap = dimMap + (size -> sizeMap)\n",
    "        timeCPALSCoordinateMatrix(dimension) = dimMap\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T18:24:18.679803Z",
     "start_time": "2021-02-20T18:24:18.087Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.6`\n",
    "import com.github.tototoshi.csv._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T21:06:07.068406Z",
     "start_time": "2021-02-20T21:06:06.726Z"
    }
   },
   "outputs": [],
   "source": [
    "val f = new java.io.File(s\"\"\"results/timeCPALSMuLOT.csv\"\"\")\n",
    "val fileExists = f.exists()\n",
    "val writer = CSVWriter.open(f, append = true)\n",
    "if (!fileExists) {\n",
    "    writer.writeRow(List[String](\"dimension\", \"size\", \"sparsity\", \"time\"))\n",
    "}\n",
    "for ((dimension, r1) <- timeCPALSCoordinateMatrix; (size, r2) <- r1; (sparsity, time) <- r2) {\n",
    "    println(List[Any](dimension, size, sparsity, time))\n",
    "    writer.writeRow(List[Any](dimension, size, sparsity, time))\n",
    "}\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
