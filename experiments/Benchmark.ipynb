{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "Load Spark and the jar containing the ExtendedBlockMatrix, and starts the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T07:15:54.944675Z",
     "start_time": "2020-12-26T07:15:49.093Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.1`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T07:15:56.024188Z",
     "start_time": "2020-12-26T07:15:49.994Z"
    }
   },
   "outputs": [],
   "source": [
    "val currentDirectory = new java.io.File(\".\").getCanonicalPath\n",
    "val path = java.nio.file.FileSystems.getDefault().getPath(s\"$currentDirectory/lib/sparktensordecomposition_2.12-0.1.jar\")\n",
    "val x = ammonite.ops.Path(path)\n",
    "interp.load.cp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T07:15:56.606608Z",
     "start_time": "2020-12-26T07:15:50.844Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.mllib.linalg.distributed.ExtendedBlockMatrix\n",
    "import org.apache.spark.mllib.linalg.distributed.ExtendedBlockMatrix._\n",
    "import tensordecomposition._\n",
    "import tensordecomposition.CPALS._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T07:15:59.136725Z",
     "start_time": "2020-12-26T07:15:51.744Z"
    }
   },
   "outputs": [],
   "source": [
    "implicit val spark = {\n",
    "    val MAX_MEMORY = \"126g\"\n",
    "    SparkSession.builder()\n",
    "        .config(\"spark.executor.memory\", MAX_MEMORY)\n",
    "        .config(\"spark.driver.memory\", MAX_MEMORY)\n",
    "        .appName(\"BenchmarkSparkCPALS\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "}\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sparkContext.setCheckpointDir(\"Checkpoint\")\n",
    "import spark.implicits._\n",
    "\n",
    "spark.sparkContext.getConf.getAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP-ALS\n",
    "Create the function used to perform the CP decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T22:44:12.870400Z",
     "start_time": "2020-12-27T22:44:11.606Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeCPALS(tensor: Tensor, rank: Int, maxIterations: Int = 5)\n",
    "        (implicit spark: SparkSession): Kruskal = {\n",
    "    val tensorMatricized = tensor.matricization()\n",
    "    tensorMatricized.map(m => {\n",
    "        val mc = m.cache()\n",
    "        mc.checkpoint()\n",
    "        mc\n",
    "    })\n",
    "    val result = new Array[ExtendedBlockMatrix](tensor.order)                   \n",
    "    var lambda = new Array[Double](tensor.order)\n",
    "    // Randomized initialization\n",
    "    for (i <- 1 until tensor.order) {\n",
    "        result(i) = ExtendedBlockMatrix.gaussian(tensor.dimensionsSize(i), rank)\n",
    "    }\n",
    "    // V is updated for each dimension rather than recalculated\n",
    "    var v = (for (k <- 1 until result.size) yield\n",
    "        result(k)).reduce((m1, m2) => (m1.transpose.multiply(m1)).hadamard(m2.transpose.multiply(m2)))\n",
    "    var termination = false\n",
    "    var nbIterations = 1\n",
    "    while (!termination) {\n",
    "        println(\"iteration \" + nbIterations)\n",
    "        for (i <- 0 until tensor.order) {\n",
    "            // Remove current dimension from V\n",
    "            if (result(i) != null) {\n",
    "                v = v.hadamard(result(i).transpose.multiply(result(i)), (m1, m2) => m1 /:/ m2)\n",
    "            }\n",
    "            // Compute MTTKRP\n",
    "            val mttkrp = ExtendedBlockMatrix.mttkrp(tensorMatricized(i),\n",
    "                    (for (k <- 0 until result.size if i != k) yield result(k)).toArray,\n",
    "                    (for (k <- 0 until tensor.dimensionsSize.size if i != k) yield tensor.dimensionsSize(k)).toArray,\n",
    "                    tensor.dimensionsSize(i),\n",
    "                    rank\n",
    "                )\n",
    "            result(i) = mttkrp.multiply(v.pinverse())\n",
    "\n",
    "            // Compute lambda\n",
    "            lambda = result(i).norm()\n",
    "            result(i) = result(i).applyOperation(m => {\n",
    "                for (k <- 0 until rank) {\n",
    "                    m(::,k) := m(::,k) / lambda(k)\n",
    "                }\n",
    "                m\n",
    "            })\n",
    "\n",
    "            // Update of V\n",
    "            v = v.hadamard(result(i).transpose.multiply(result(i)))\n",
    "        }\n",
    "\n",
    "        if (nbIterations >= maxIterations) {\n",
    "            termination = true\n",
    "        } else {\n",
    "            nbIterations += 1\n",
    "        }\n",
    "    }\n",
    "    Kruskal(result, lambda)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Load the CSV files previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T07:16:00.915831Z",
     "start_time": "2020-12-26T07:15:58.228Z"
    }
   },
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "\n",
    "val tensorsFiles = new File(\"sample_tensors\").listFiles\n",
    "    .map(_.getName)\n",
    "    .filter(f => f.startsWith(\"tensor\") && !f.contains(\"clusters\")).toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T07:16:08.945122Z",
     "start_time": "2020-12-26T07:16:00.268Z"
    }
   },
   "outputs": [],
   "source": [
    "val tensors = (for (tensorFile <- tensorsFiles) yield {\n",
    "    val name = tensorFile.replace(\".csv\", \"\").replace(\"tensor_\", \"\").split(\"_\")\n",
    "    val nbDimensions = name(0).toInt\n",
    "    val size = name(1).toLong\n",
    "    (tensorFile -> Tensor.fromIndexedDataFrame(\n",
    "        spark.read.option(\"header\",true).csv(s\"sample_tensors/$tensorFile\"), \n",
    "        (for (i <- 0 until nbDimensions) yield size)toList))\n",
    "}).toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CP\n",
    "Execute the CP on the loaded tensors, and measure the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T07:16:09.236212Z",
     "start_time": "2020-12-26T07:16:06.215Z"
    }
   },
   "outputs": [],
   "source": [
    "import scala.collection.mutable.{Map => MMap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T09:40:01.593715Z",
     "start_time": "2020-12-29T18:42:30.874Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var timeCPALSCoordinateMatrix = MMap[Int, MMap[Int, MMap[Double, Int]]]()\n",
    "\n",
    "for (dimension <- 3 to 5; \n",
    "     size <- List(100, 1000, 10000, 100000);\n",
    "     sparsity <- List(1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10)) {\n",
    "    val fileName = s\"tensor_${dimension}_${size}_${sparsity}.csv\"\n",
    "    if (tensors.contains(fileName)) {\n",
    "        println(fileName)\n",
    "        val nbIterations = 5\n",
    "        var endTime = 0\n",
    "        for (j <- 0 until nbIterations) {        \n",
    "            val startTime = System.currentTimeMillis()\n",
    "\n",
    "            computeCPALSWithCoordinateMatrix(tensors.get(fileName).get, 3)\n",
    "\n",
    "            endTime += (System.currentTimeMillis() - startTime).toInt\n",
    "\n",
    "            println(\"Execution time: \" + (endTime / 1000) + \"s\")\n",
    "        }\n",
    "        val finalTime = (endTime / nbIterations).toInt\n",
    "        var dimMap = timeCPALSCoordinateMatrix.getOrElse(dimension, MMap[Int, MMap[Double, Int]]())\n",
    "        var sizeMap = dimMap.getOrElse(size, MMap[Double, Int]())\n",
    "        sizeMap = sizeMap + (sparsity -> finalTime)\n",
    "        dimMap = dimMap + (size -> sizeMap)\n",
    "        timeCPALSCoordinateMatrix(dimension) = dimMap\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T15:25:41.583133Z",
     "start_time": "2020-12-30T15:25:27.672Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.2`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T15:25:43.766341Z",
     "start_time": "2020-12-30T15:25:37.095Z"
    }
   },
   "outputs": [],
   "source": [
    "for ((dimension, r1) <- timeCPALSCoordinateMatrix) {\n",
    "    var plot = Seq[Scatter]()\n",
    "    for (size <- r1.keys.toSeq.sorted) {\n",
    "        val r2 = r1(size)\n",
    "        val sparsity = r2.keys.toSeq.sorted.toList.reverse\n",
    "        plot = plot :+ Scatter(sparsity.map(_ * math.pow(size, dimension)), for (key <- sparsity) yield r2(key), name = s\"Size $size\")\n",
    "    }\n",
    "    plot.plot(title = s\"Number of dimensions: $dimension\", \n",
    "          xaxis = Axis(title = \"nnz\", `type` = AxisType.Log),\n",
    "          yaxis = Axis(title = \"Time\", `type` = AxisType.Log))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
