{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:36.394059Z",
     "start_time": "2021-02-21T08:57:33.231Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:1.5.2`\n",
    "import $ivy.`org.apache.spark::spark-mllib:1.5.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:37.319301Z",
     "start_time": "2021-02-21T08:57:33.896Z"
    }
   },
   "outputs": [],
   "source": [
    "val currentDirectory = new java.io.File(\".\").getCanonicalPath\n",
    "val path = java.nio.file.FileSystems.getDefault().getPath(s\"$currentDirectory/lib/cstf_2.11-0.1.jar\")\n",
    "val x = ammonite.ops.Path(path)\n",
    "interp.load.cp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:38.091621Z",
     "start_time": "2021-02-21T08:57:34.588Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.paramath.CSTF._\n",
    "import org.paramath.CSTF.utils.CSTFUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:41.238994Z",
     "start_time": "2021-02-21T08:57:35.364Z"
    }
   },
   "outputs": [],
   "source": [
    "val MAX_MEMORY = \"126g\"\n",
    "val sparkConf = new SparkConf()\n",
    "        .setMaster(\"local[*]\")\n",
    "        .setAppName(\"BenchmarkCSTF\")\n",
    "        .set(\"spark.executor.memory\", MAX_MEMORY)\n",
    "        .set(\"spark.driver.memory\", MAX_MEMORY)\n",
    "        .set(\"spark.executor.heartbeatInterval\", \"10000000000\")\n",
    "implicit val sc = new SparkContext(sparkConf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to run CP-ALS with CSTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:41.618772Z",
     "start_time": "2021-02-21T08:57:38.085Z"
    }
   },
   "outputs": [],
   "source": [
    "def RDDStringToRDDVector(rdd: RDD[String])(implicit sc: SparkContext): RDD[Vector] = {\n",
    "    rdd.map(e => Vectors.dense(e.split(\"\\t\").map(_.toDouble)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:42.081437Z",
     "start_time": "2021-02-21T08:57:39.438Z"
    }
   },
   "outputs": [],
   "source": [
    "case class TensorRDD(rdd: RDD[String], order: Int, dimensionsSize: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:42.477167Z",
     "start_time": "2021-02-21T08:57:40.467Z"
    }
   },
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "\n",
    "val tensorsFiles = new File(\"sample_tensors_HaTen\").listFiles\n",
    "    .map(_.getName)\n",
    "    .filter(f => f.startsWith(\"tensor\") && !f.contains(\"clusters\")).toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:43.642007Z",
     "start_time": "2021-02-21T08:57:41.757Z"
    }
   },
   "outputs": [],
   "source": [
    "val tensors = (for (tensorFile <- tensorsFiles) yield {\n",
    "    val name = tensorFile.replace(\".csv\", \"\").replace(\"tensor_\", \"\").split(\"_\")\n",
    "    val nbDimensions = name(0).toInt\n",
    "    val size = name(1).toLong\n",
    "    (tensorFile -> \n",
    "        TensorRDD(sc.textFile(s\"sample_tensors_HaTen/$tensorFile\"), nbDimensions, size.toInt))\n",
    "}).toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:57:44.376663Z",
     "start_time": "2021-02-21T08:57:44.148Z"
    }
   },
   "outputs": [],
   "source": [
    "import scala.collection.mutable.{Map => MMap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-21T09:13:16.742Z"
    }
   },
   "outputs": [],
   "source": [
    "var timeCPALSCSTF = MMap[Int, MMap[Int, MMap[Double, Int]]]()\n",
    "\n",
    "for (dimension <- 3 to 3; \n",
    "     size <- List(100, 1000, 10000, 100000);\n",
    "     sparsity <- List(1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10).reverse) {\n",
    "    val fileName = s\"tensor_${dimension}_${size}_${sparsity}.csv\"\n",
    "    if (tensors.contains(fileName)) {\n",
    "        println(fileName)\n",
    "        val nbIterations = 1\n",
    "        var endTime = 0\n",
    "        for (j <- 0 until nbIterations) {\n",
    "            val _t = tensors.get(fileName).get\n",
    "            val tensor = RDDStringToRDDVector(_t.rdd)\n",
    "            tensor.count()\n",
    "            \n",
    "            val startTime = System.currentTimeMillis()\n",
    "\n",
    "            COOGeneralizedSingleVec.CP_ALS(tensor, 5, 3, 1e-100, sc)\n",
    "            //COOGeneralizedRowMatrix.CP_ALS(tensor, 5, 3, 1e-100, sc)\n",
    "            //COOGeneralized.CP_ALS(tensor, 5, 3, 1e-100, sc)\n",
    "            \n",
    "            endTime += (System.currentTimeMillis() - startTime).toInt\n",
    "\n",
    "            println(\"Execution time: \" + (endTime / 1000) + \"s\")\n",
    "        }\n",
    "        val finalTime = (endTime / nbIterations).toInt\n",
    "        var dimMap = timeCPALSCSTF.getOrElse(dimension, MMap[Int, MMap[Double, Int]]())\n",
    "        var sizeMap = dimMap.getOrElse(size, MMap[Double, Int]())\n",
    "        sizeMap = sizeMap + (sparsity -> finalTime)\n",
    "        dimMap = dimMap + (size -> sizeMap)\n",
    "        timeCPALSCSTF(dimension) = dimMap\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T14:09:01.925331Z",
     "start_time": "2021-02-17T14:09:01.409Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.6`\n",
    "import com.github.tototoshi.csv._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T10:18:46.825785Z",
     "start_time": "2021-02-18T10:18:46.555Z"
    }
   },
   "outputs": [],
   "source": [
    "val f = new java.io.File(s\"\"\"results/benchmarkCSTF.csv\"\"\")\n",
    "val fileExists = f.exists()\n",
    "val writer = CSVWriter.open(f, append = true)\n",
    "if (!fileExists) {\n",
    "    writer.writeRow(List[String](\"dimension\", \"size\", \"sparsity\", \"time\"))\n",
    "}\n",
    "for ((dimension, r1) <- timeCPALSCSTF; (size, r2) <- r1; (sparsity, time) <- r2) {\n",
    "    println(List[Any](dimension, size, sparsity, time))\n",
    "    writer.writeRow(List[Any](dimension, size, sparsity, time))\n",
    "}\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Scala (2.11.12)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
