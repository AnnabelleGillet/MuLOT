{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:08:52.039378Z",
     "start_time": "2021-12-13T14:08:45.426Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.1`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.0.1`\n",
    "import $ivy.`org.scalanlp::breeze:1.1`\n",
    "import $ivy.`org.scalanlp::breeze-natives:1.1`\n",
    "import $ivy.`org.postgresql:postgresql:42.2.5`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:08:53.049316Z",
     "start_time": "2021-12-13T14:08:45.429Z"
    }
   },
   "outputs": [],
   "source": [
    "val currentDirectory = new java.io.File(\".\").getCanonicalPath\n",
    "val path = java.nio.file.FileSystems.getDefault().getPath(s\"$currentDirectory/lib/mulot_2.12-0.3.jar\")\n",
    "val x = ammonite.ops.Path(path)\n",
    "interp.load.cp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:08:53.774740Z",
     "start_time": "2021-12-13T14:08:45.431Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import mulot.Tensor\n",
    "import mulot.tensordecomposition._\n",
    "import mulot.tensordecomposition.CPALS._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:08:56.344878Z",
     "start_time": "2021-12-13T14:08:45.432Z"
    }
   },
   "outputs": [],
   "source": [
    "implicit val spark = {\n",
    "    val MAX_MEMORY = \"64g\"\n",
    "    SparkSession.builder()\n",
    "        .config(\"spark.executor.memory\", MAX_MEMORY)\n",
    "        .config(\"spark.driver.memory\", MAX_MEMORY)\n",
    "        .appName(s\"Strata\")\n",
    "        .master(\"local[40]\")\n",
    "        .getOrCreate()\n",
    "}\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sparkContext.setCheckpointDir(\"Checkpoint\")\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratification\n",
    "To compute the stratification, two functions are needed:\n",
    "* One to find the best rank of the CP decomposition (according to CORCONDIA)\n",
    "* One to perform the stratification: the best rank is used, the elements of each resulting rank are kept only is they are higher of the average value of their vector, and the clusters are removed from the tensor (deflation) to iterate on the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:08:57.058541Z",
     "start_time": "2021-12-13T14:08:45.434Z"
    }
   },
   "outputs": [],
   "source": [
    "case class BestDecomposition(kruskal: Kruskal, rank: Int)\n",
    "\n",
    "/**\n",
    " * Find the best rank for this tensor, according to the core consistency (CORCONDIA).\n",
    " *\n",
    " * @param tensor the [[Tensor]] on which to perform the decomposition\n",
    " * @param norm\n",
    " * @param maxIterations\n",
    " * @param minFms\n",
    " * @param checkpoint\n",
    " * @param highRank\n",
    " * @param hintRank\n",
    " * @param spark\n",
    " * @return\n",
    " */\n",
    "def findBestDecomposition(tensor: Tensor, norm: String = NORM_L1, maxIterations: Int = 25, minFms: Double = 0.99, \n",
    "                          highRank: Option[Boolean] = None, hintRank: Int = 20)\n",
    "                          (implicit spark: SparkSession): BestDecomposition = {\n",
    "    var currentRank = 2\n",
    "    var minRank = 2\n",
    "    var maxRank = Integer.MAX_VALUE\n",
    "    println(s\"Try rank $currentRank\")\n",
    "    var previousDecomposition = CPALS.compute(tensor, currentRank, norm, maxIterations, minFms, highRank, true)\n",
    "    var bestDecomposition = BestDecomposition(previousDecomposition, currentRank)\n",
    "    \n",
    "    println(s\"Try rank $hintRank\")\n",
    "    var currentDecomposition = CPALS.compute(tensor, hintRank, norm, maxIterations, minFms, highRank, true)\n",
    "    \n",
    "    if (currentDecomposition.corcondia.get >= bestDecomposition.kruskal.corcondia.get || currentDecomposition.corcondia.get >= 80) {\n",
    "        bestDecomposition = BestDecomposition(currentDecomposition, hintRank)\n",
    "        // The hint rank is less than the best rank: keep increasing the rank\n",
    "        currentRank = hintRank\n",
    "        while (!currentDecomposition.corcondia.get.isNaN &&\n",
    "                (currentDecomposition.corcondia.get >= previousDecomposition.corcondia.get || currentDecomposition.corcondia.get >= 80)) {\n",
    "            minRank = currentRank\n",
    "            currentRank += hintRank\n",
    "            previousDecomposition = currentDecomposition\n",
    "            println(s\"Try rank $currentRank\")\n",
    "            currentDecomposition = CPALS.compute(tensor, currentRank, norm, maxIterations, minFms, highRank, true)\n",
    "            if (currentDecomposition.corcondia.get >= bestDecomposition.kruskal.corcondia.get) {\n",
    "                bestDecomposition = BestDecomposition(currentDecomposition, currentRank)\n",
    "            }\n",
    "        }\n",
    "        // Max rank found\n",
    "        maxRank = currentRank\n",
    "    } else {\n",
    "        // The hint rank is more than the best rank: set the hint rank as max rank\n",
    "        maxRank = hintRank\n",
    "        currentRank = hintRank\n",
    "    }\n",
    "\n",
    "    // Reduce the maxRank and increase the minRank until finding the best rank\n",
    "    while ((maxRank - minRank) > 1) {\n",
    "        val previousRank = currentRank\n",
    "        currentRank = minRank + ((maxRank - minRank) / 2)\n",
    "        println(s\"Try rank $currentRank\")\n",
    "        previousDecomposition = currentDecomposition\n",
    "        currentDecomposition = CPALS.compute(tensor, currentRank, norm, maxIterations, minFms, highRank, true)\n",
    "        if (currentDecomposition.corcondia.get.isNaN || currentDecomposition.corcondia.get < 0) {\n",
    "            maxRank = currentRank\n",
    "        } else {\n",
    "            if (currentDecomposition.corcondia.get >= 80) {\n",
    "                minRank = currentRank\n",
    "            } else {\n",
    "                if (previousDecomposition.corcondia.get.isNaN || currentDecomposition.corcondia.get >= previousDecomposition.corcondia.get) {\n",
    "                    if (previousRank > currentRank) {\n",
    "                        // The rank is too high\n",
    "                        maxRank = currentRank\n",
    "                    } else {\n",
    "                        // The rank is too low\n",
    "                        minRank = currentRank\n",
    "                    }\n",
    "                } else {\n",
    "                    if (previousRank > currentRank) {\n",
    "                        // The rank is too low\n",
    "                        minRank = currentRank\n",
    "                    } else {\n",
    "                        // The rank is too high\n",
    "                        maxRank = currentRank\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if (currentDecomposition.corcondia.get >= bestDecomposition.kruskal.corcondia.get ||\n",
    "            (currentDecomposition.corcondia.get >= 80 && currentRank > bestDecomposition.rank)) {\n",
    "            bestDecomposition = BestDecomposition(currentDecomposition, currentRank)\n",
    "        }\n",
    "        println(s\"Min rank: $minRank, max rank: $maxRank\")\n",
    "    }\n",
    "    // Check if best rank is 1\n",
    "    if (bestDecomposition.rank == 2 && (bestDecomposition.kruskal.corcondia.get.isNaN || bestDecomposition.kruskal.corcondia.get < 99)) {\n",
    "        println(\"Choose rank 1\")\n",
    "        val rank1Decomposition = CPALS.compute(tensor, 1, norm, maxIterations, minFms, highRank, true)\n",
    "        bestDecomposition = BestDecomposition(rank1Decomposition, 1)\n",
    "    }\n",
    "\n",
    "    bestDecomposition\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:08:58.673073Z",
     "start_time": "2021-12-13T14:08:45.436Z"
    }
   },
   "outputs": [],
   "source": [
    "import breeze.linalg.min\n",
    "import breeze.numerics.abs\n",
    "import breeze.stats.mean\n",
    "import mulot.Tensor\n",
    "import mulot.tensordecomposition.CPALS\n",
    "import org.apache.spark.sql.{Column, DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types.{DoubleType, LongType, StructField, StructType}\n",
    "\n",
    "object Strata {\n",
    "    case class Community(data: Map[String, DataFrame], size: Map[String, Int])\n",
    "    case class Stratum(communities: List[Community], depth: Int)\n",
    "\n",
    "    def compute(_tensor: Tensor, nbStrata: Int = 2, staticDimensions: List[String] = List[String](),\n",
    "                norm: String = CPALS.NORM_L1)\n",
    "               (implicit spark: SparkSession): List[Stratum] = {\n",
    "        var tensor = _tensor\n",
    "        val _valueColumnName = tensor.valueColumnName\n",
    "        var strata = List[Stratum]()\n",
    "        var numStratum = 0\n",
    "\n",
    "        while (numStratum < nbStrata) {\n",
    "            val begin = System.currentTimeMillis()\n",
    "\n",
    "            // Choose the best rank for the decomposition with CORCONDIA\n",
    "            val decomposition = findBestDecomposition(tensor, norm = norm)\n",
    "            var communities = List[Community]()\n",
    "            var conditions = List[Column]()\n",
    "\n",
    "            for (rank <- 0 until decomposition.rank) {\n",
    "\n",
    "                val communityData = new Array[List[(Int, Double)]](tensor.order)\n",
    "                val communitySize = new Array[Int](tensor.order)\n",
    "                val factorVectors = decomposition.kruskal.A.map(m => abs(m.toSparseBreeze().toDense(::, rank)))\n",
    "                val averageValues = for (vector <- factorVectors) yield { mean(vector) - min(vector) }\n",
    "                println(averageValues.mkString(\" \"))\n",
    "                for (i <- factorVectors.indices) {\n",
    "                    if (staticDimensions.contains(tensor.dimensionsName(i))) {\n",
    "                        // If the dimension is static, we get all the elements...\n",
    "                        communityData(i) = factorVectors(i).mapPairs((i, v) => (i, v)).toArray.toList\n",
    "                    } else {\n",
    "                        // ...if not, we keep only the elements with a value above the threshold\n",
    "                        communityData(i) = factorVectors(i).findAll(_ >= averageValues(i)).map(index => (index, factorVectors(i)(index))).toList\n",
    "                    }\n",
    "                    \n",
    "                    communitySize(i) = communityData(i).length\n",
    "                }\n",
    "\n",
    "                // Add the community to the other communities\n",
    "                val communityDf = {\n",
    "                    (for (i <- tensor.dimensionsName.indices) yield {\n",
    "                        var df: DataFrame = spark.createDataFrame(\n",
    "                            spark.sparkContext.parallelize(communityData(i).map(e => Row(e._1.toLong, e._2))),\n",
    "                            StructType(Array(StructField(\"dimIndex\", LongType, nullable = true),\n",
    "                                StructField(\"value\", DoubleType, nullable = true)))\n",
    "                        )\n",
    "\n",
    "                        if (tensor.dimensionsIndex.isDefined) {\n",
    "                            df = df.join(tensor.dimensionsIndex.get(i), \"dimIndex\").drop(\"dimIndex\")\n",
    "                            df = df.withColumnRenamed(\"dimValue\", tensor.dimensionsName(i))\n",
    "                        } else {\n",
    "                            df = df.withColumnRenamed(\"dimIndex\", tensor.dimensionsName(i))\n",
    "                        }\n",
    "                        tensor.dimensionsName(i) -> df\n",
    "                    }).toMap\n",
    "                }\n",
    "\n",
    "                communities :+= Community(communityDf, Map[String, Int]((for (i <- communitySize.indices) yield tensor.dimensionsName(i) -> communitySize(i)):_*))\n",
    "\n",
    "                // Remove the community from the tensor\n",
    "                var conditionIndex = 0\n",
    "                while (staticDimensions.contains(tensor.dimensionsName(conditionIndex))) {\n",
    "                    conditionIndex += 1\n",
    "                }\n",
    "                var condition = col(s\"row_$conditionIndex\").isInCollection(communityData(conditionIndex).map(v => v._1))\n",
    "                conditionIndex += 1\n",
    "                for (i <- conditionIndex until communityData.length if !staticDimensions.contains(tensor.dimensionsName(i))) {\n",
    "                    condition = condition and col(s\"row_$i\").isInCollection(communityData(i).map(v => v._1))\n",
    "                }\n",
    "                conditions :+= condition\n",
    "            }\n",
    "\n",
    "            for (c <- communities) {\n",
    "                println(s\"Community found: ${for (i <- c.size.keys) yield s\"$i: ${c.size(i)}\"}\")\n",
    "            }\n",
    "\n",
    "            // Add the stratum of communities\n",
    "            strata :+= Stratum(communities, numStratum)\n",
    "            numStratum += 1\n",
    "\n",
    "            // Deflate the tensor\n",
    "            var tensorData = tensor.data.filter(!conditions(0))\n",
    "            for (condition <- conditions.tail) {\n",
    "                tensorData = tensorData.filter(!condition)\n",
    "            }\n",
    "\n",
    "            // Keep the same dimensions name\n",
    "            for (i <- tensor.dimensionsName.indices) {\n",
    "                if (tensor.dimensionsIndex.isDefined) {\n",
    "                    tensorData = tensorData.withColumnRenamed(s\"row_$i\", \"dimIndex\").join(tensor.dimensionsIndex.get(i), \"dimIndex\").drop(\"dimIndex\")\n",
    "                    tensorData = tensorData.withColumnRenamed(\"dimValue\", tensor.dimensionsName(i))\n",
    "                } else {\n",
    "                    tensorData = tensorData.withColumnRenamed(s\"row_$i\", tensor.dimensionsName(i))\n",
    "                }\n",
    "            }\n",
    "            tensor = Tensor(tensorData.localCheckpoint().cache(), _valueColumnName)\n",
    "\n",
    "            println(s\"Stratum found in ${(System.currentTimeMillis() - begin).toDouble / 1000.0}s\")\n",
    "            }\n",
    "        strata\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of the stratification\n",
    "The tensor User-Hashtag-Time is loaded from the CSV file, and the stratification method is used on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:09:03.878733Z",
     "start_time": "2021-12-13T14:08:45.438Z"
    }
   },
   "outputs": [],
   "source": [
    "val dfUserHashtagTime = spark.read.options(Map(\"inferSchema\"->\"true\",\"header\"->\"true\")).csv(\"UHT.csv\")\n",
    "dfUserHashtagTime.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:09:07.743562Z",
     "start_time": "2021-12-13T14:08:45.439Z"
    }
   },
   "outputs": [],
   "source": [
    "val tensorUserHashtagTime = Tensor(dfUserHashtagTime)\n",
    "tensorUserHashtagTime.dimensionsSize.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:11:53.688499Z",
     "start_time": "2021-12-13T14:08:45.442Z"
    }
   },
   "outputs": [],
   "source": [
    "val communitiesUHT = Strata.compute(tensorUserHashtagTime, 3, staticDimensions = List[String](\"time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:21:30.623732Z",
     "start_time": "2021-12-13T14:21:30.275Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._\n",
    "import org.apache.spark.sql.functions.{abs => sparkAbs}\n",
    "\n",
    "def plot3D(df: Map[String, DataFrame], rank: Int, d1: String, d2: String, \n",
    "           time: String = \"time\", nbDays: Int = 1, lambda: Double = 0.0, valueColumn: String = \"value\") = {\n",
    "    var plot = Seq(\n",
    "        Scatter(\n",
    "            df(time)\n",
    "                .sort(col(time)).select(to_date(from_unixtime((col(time) * 3600 * 24 * nbDays)))).collect.map(_.get(0).toString).toSeq, \n",
    "            df(time)\n",
    "                .sort(col(time)).select(sparkAbs(col(valueColumn))).collect.map(_.getDouble(0)).toSeq,\n",
    "            name = \"Time\",\n",
    "            xaxis = AxisReference.X1,\n",
    "            yaxis = AxisReference.Y1\n",
    "        ),\n",
    "        Bar(\n",
    "            df(d1)\n",
    "                .sort(sparkAbs(col(valueColumn)).desc).limit(20).select(d1).collect.map(\"u\" + _.getInt(0)).toSeq,\n",
    "            df(d1)\n",
    "                .sort(sparkAbs(col(valueColumn)).desc).limit(20).select(sparkAbs(col(valueColumn))).collect.map(_.getDouble(0)).toSeq,\n",
    "            name = d1.capitalize,\n",
    "            xaxis = AxisReference.X2,\n",
    "            yaxis = AxisReference.Y2\n",
    "        ),\n",
    "        Bar(\n",
    "            df(d2)\n",
    "                .sort(sparkAbs(col(valueColumn)).desc).limit(20).select(d2).collect.map(_.getString(0)).toSeq,\n",
    "            df(d2)\n",
    "                .sort(sparkAbs(col(valueColumn)).desc).limit(20).select(sparkAbs(col(valueColumn))).collect.map(_.getDouble(0)).toSeq,\n",
    "            name = d2.capitalize,\n",
    "            xaxis = AxisReference.X3,\n",
    "            yaxis = AxisReference.Y3\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    val lambdaText = if (lambda > 0.0) s\"lambda = $lambda\" else \"\"\n",
    "    \n",
    "    val layout = Layout(\n",
    "        title = s\"Rank $rank $lambdaText\",\n",
    "        width = 1000,\n",
    "        xaxis1 = Axis(anchor = AxisAnchor.Reference(AxisReference.Y1), domain = (0.0, 1.0), automargin = true),\n",
    "        xaxis2 = Axis(anchor = AxisAnchor.Reference(AxisReference.Y2), domain = (0.0, 0.49), automargin = true),\n",
    "        xaxis3 = Axis(anchor = AxisAnchor.Reference(AxisReference.Y3), domain = (0.51, 1.0), automargin = true),\n",
    "        yaxis1 = Axis(anchor = AxisAnchor.Reference(AxisReference.X1), domain = (0.55, 1.0), automargin = true),\n",
    "        yaxis2 = Axis(anchor = AxisAnchor.Reference(AxisReference.X2), domain = (0.0, 0.45), automargin = true),\n",
    "        yaxis3 = Axis(anchor = AxisAnchor.Reference(AxisReference.X3), domain = (0.0, 0.45), automargin = true),\n",
    "        legend = Legend(y = 1.1, x = .5, yanchor = Anchor.Top, xanchor = Anchor.Center, orientation = Orientation.Horizontal)\n",
    "    )\n",
    "    \n",
    "    plot.plot(layout = layout, Config(), \"\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T14:21:48.006150Z",
     "start_time": "2021-12-13T14:21:32.203Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val stratum = 0\n",
    "for (rank <- communitiesUHT(stratum).communities.indices) {\n",
    "    plot3D(communitiesUHT(stratum).communities(rank).data, rank, \"user\", \"hashtag\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
